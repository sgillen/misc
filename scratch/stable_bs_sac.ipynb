{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sgillen/work/external/stable-baselines/stable_baselines/common/tf_util.py:57: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sgillen/work/external/stable-baselines/stable_baselines/common/tf_util.py:66: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sgillen/work/external/stable-baselines/stable_baselines/sac/sac.py:154: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sgillen/work/external/stable-baselines/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sgillen/work/external/stable-baselines/stable_baselines/sac/policies.py:213: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /Users/sgillen/anaconda3/envs/stable/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /Users/sgillen/work/external/stable-baselines/stable_baselines/sac/policies.py:49: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /Users/sgillen/work/external/stable-baselines/stable_baselines/sac/policies.py:235: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sgillen/work/external/stable-baselines/stable_baselines/sac/policies.py:81: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sgillen/work/external/stable-baselines/stable_baselines/sac/sac.py:209: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sgillen/work/external/stable-baselines/stable_baselines/sac/sac.py:245: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sgillen/work/external/stable-baselines/stable_baselines/common/tf_util.py:312: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sgillen/work/external/stable-baselines/stable_baselines/common/tf_util.py:312: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sgillen/anaconda3/envs/stable/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/sgillen/work/external/stable-baselines/stable_baselines/sac/sac.py:281: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sgillen/work/external/stable-baselines/stable_baselines/sac/sac.py:308: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sgillen/work/external/stable-baselines/stable_baselines/sac/sac.py:325: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sgillen/work/external/stable-baselines/stable_baselines/sac/sac.py:328: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "--------------------------------------\n",
      "| current_lr              | 0.0003   |\n",
      "| episodes                | 10       |\n",
      "| fps                     | 4003     |\n",
      "| mean 100 episode reward | 4.3      |\n",
      "| n_updates               | 0        |\n",
      "| time_elapsed            | 0        |\n",
      "| total timesteps         | 39       |\n",
      "--------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0003       |\n",
      "| ent_coef                | 0.9964003    |\n",
      "| ent_coef_loss           | -0.006063347 |\n",
      "| entropy                 | 1.3650123    |\n",
      "| episodes                | 20           |\n",
      "| fps                     | 184          |\n",
      "| mean 100 episode reward | 5.9          |\n",
      "| n_updates               | 13           |\n",
      "| policy_loss             | -1.1028988   |\n",
      "| qf1_loss                | 0.18871984   |\n",
      "| qf2_loss                | 0.4113233    |\n",
      "| time_elapsed            | 0            |\n",
      "| total timesteps         | 113          |\n",
      "| value_loss              | 0.1854897    |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0003       |\n",
      "| ent_coef                | 0.982409     |\n",
      "| ent_coef_loss           | -0.030266102 |\n",
      "| entropy                 | 1.3459237    |\n",
      "| episodes                | 30           |\n",
      "| fps                     | 200          |\n",
      "| mean 100 episode reward | 5.5          |\n",
      "| n_updates               | 60           |\n",
      "| policy_loss             | -1.3174247   |\n",
      "| qf1_loss                | 0.12661731   |\n",
      "| qf2_loss                | 0.11640397   |\n",
      "| time_elapsed            | 0            |\n",
      "| total timesteps         | 160          |\n",
      "| value_loss              | 0.17054558   |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.96919435  |\n",
      "| ent_coef_loss           | -0.0527366  |\n",
      "| entropy                 | 1.3595359   |\n",
      "| episodes                | 40          |\n",
      "| fps                     | 209         |\n",
      "| mean 100 episode reward | 5.3         |\n",
      "| n_updates               | 105         |\n",
      "| policy_loss             | -1.536515   |\n",
      "| qf1_loss                | 0.13700004  |\n",
      "| qf2_loss                | 0.15567839  |\n",
      "| time_elapsed            | 0           |\n",
      "| total timesteps         | 205         |\n",
      "| value_loss              | 0.120730355 |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.95304763  |\n",
      "| ent_coef_loss           | -0.08111441 |\n",
      "| entropy                 | 1.3628039   |\n",
      "| episodes                | 50          |\n",
      "| fps                     | 219         |\n",
      "| mean 100 episode reward | 5.3         |\n",
      "| n_updates               | 161         |\n",
      "| policy_loss             | -1.8008381  |\n",
      "| qf1_loss                | 0.1376101   |\n",
      "| qf2_loss                | 0.16838023  |\n",
      "| time_elapsed            | 1           |\n",
      "| total timesteps         | 261         |\n",
      "| value_loss              | 0.074907705 |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.93271154  |\n",
      "| ent_coef_loss           | -0.11578037 |\n",
      "| entropy                 | 1.3632994   |\n",
      "| episodes                | 60          |\n",
      "| fps                     | 228         |\n",
      "| mean 100 episode reward | 5.6         |\n",
      "| n_updates               | 233         |\n",
      "| policy_loss             | -2.0353005  |\n",
      "| qf1_loss                | 0.11060454  |\n",
      "| qf2_loss                | 0.1503084   |\n",
      "| time_elapsed            | 1           |\n",
      "| total timesteps         | 333         |\n",
      "| value_loss              | 0.056381688 |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.91968936  |\n",
      "| ent_coef_loss           | -0.14190423 |\n",
      "| entropy                 | 1.3826495   |\n",
      "| episodes                | 70          |\n",
      "| fps                     | 232         |\n",
      "| mean 100 episode reward | 5.5         |\n",
      "| n_updates               | 280         |\n",
      "| policy_loss             | -2.313349   |\n",
      "| qf1_loss                | 0.07503824  |\n",
      "| qf2_loss                | 0.116296075 |\n",
      "| time_elapsed            | 1           |\n",
      "| total timesteps         | 380         |\n",
      "| value_loss              | 0.06467291  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.90600127  |\n",
      "| ent_coef_loss           | -0.16543072 |\n",
      "| entropy                 | 1.3735471   |\n",
      "| episodes                | 80          |\n",
      "| fps                     | 235         |\n",
      "| mean 100 episode reward | 5.4         |\n",
      "| n_updates               | 330         |\n",
      "| policy_loss             | -2.4277377  |\n",
      "| qf1_loss                | 0.08526716  |\n",
      "| qf2_loss                | 0.13583761  |\n",
      "| time_elapsed            | 1           |\n",
      "| total timesteps         | 430         |\n",
      "| value_loss              | 0.053407736 |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.89519894  |\n",
      "| ent_coef_loss           | -0.18436715 |\n",
      "| entropy                 | 1.32407     |\n",
      "| episodes                | 90          |\n",
      "| fps                     | 236         |\n",
      "| mean 100 episode reward | 5.3         |\n",
      "| n_updates               | 370         |\n",
      "| policy_loss             | -2.6009803  |\n",
      "| qf1_loss                | 0.071180016 |\n",
      "| qf2_loss                | 0.08876558  |\n",
      "| time_elapsed            | 1           |\n",
      "| total timesteps         | 470         |\n",
      "| value_loss              | 0.024459645 |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.8810459   |\n",
      "| ent_coef_loss           | -0.21293612 |\n",
      "| entropy                 | 1.3148966   |\n",
      "| episodes                | 100         |\n",
      "| fps                     | 238         |\n",
      "| mean 100 episode reward | 5.3         |\n",
      "| n_updates               | 423         |\n",
      "| policy_loss             | -2.7348852  |\n",
      "| qf1_loss                | 0.084822044 |\n",
      "| qf2_loss                | 0.099822894 |\n",
      "| time_elapsed            | 2           |\n",
      "| total timesteps         | 523         |\n",
      "| value_loss              | 0.019529004 |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.8681949   |\n",
      "| ent_coef_loss           | -0.2357247  |\n",
      "| entropy                 | 1.3136897   |\n",
      "| episodes                | 110         |\n",
      "| fps                     | 240         |\n",
      "| mean 100 episode reward | 5.3         |\n",
      "| n_updates               | 472         |\n",
      "| policy_loss             | -2.911965   |\n",
      "| qf1_loss                | 0.10053535  |\n",
      "| qf2_loss                | 0.1092778   |\n",
      "| time_elapsed            | 2           |\n",
      "| total timesteps         | 572         |\n",
      "| value_loss              | 0.017490316 |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.8506358   |\n",
      "| ent_coef_loss           | -0.2787891  |\n",
      "| entropy                 | 1.3063376   |\n",
      "| episodes                | 120         |\n",
      "| fps                     | 242         |\n",
      "| mean 100 episode reward | 5.3         |\n",
      "| n_updates               | 540         |\n",
      "| policy_loss             | -3.2100668  |\n",
      "| qf1_loss                | 0.21381289  |\n",
      "| qf2_loss                | 0.248178    |\n",
      "| time_elapsed            | 2           |\n",
      "| total timesteps         | 640         |\n",
      "| value_loss              | 0.027171094 |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.8263184   |\n",
      "| ent_coef_loss           | -0.32254913 |\n",
      "| entropy                 | 1.2604166   |\n",
      "| episodes                | 130         |\n",
      "| fps                     | 243         |\n",
      "| mean 100 episode reward | 5.8         |\n",
      "| n_updates               | 637         |\n",
      "| policy_loss             | -3.4584055  |\n",
      "| qf1_loss                | 0.17517981  |\n",
      "| qf2_loss                | 0.23022118  |\n",
      "| time_elapsed            | 3           |\n",
      "| total timesteps         | 737         |\n",
      "| value_loss              | 0.10081065  |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| ent_coef                | 0.81084335 |\n",
      "| ent_coef_loss           | -0.3420145 |\n",
      "| entropy                 | 1.264179   |\n",
      "| episodes                | 140        |\n",
      "| fps                     | 244        |\n",
      "| mean 100 episode reward | 6          |\n",
      "| n_updates               | 701        |\n",
      "| policy_loss             | -3.911679  |\n",
      "| qf1_loss                | 0.10068213 |\n",
      "| qf2_loss                | 0.13042998 |\n",
      "| time_elapsed            | 3          |\n",
      "| total timesteps         | 801        |\n",
      "| value_loss              | 0.07797503 |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.7907302   |\n",
      "| ent_coef_loss           | -0.38996628 |\n",
      "| entropy                 | 1.2113488   |\n",
      "| episodes                | 150         |\n",
      "| fps                     | 247         |\n",
      "| mean 100 episode reward | 6.3         |\n",
      "| n_updates               | 787         |\n",
      "| policy_loss             | -4.180455   |\n",
      "| qf1_loss                | 0.14957248  |\n",
      "| qf2_loss                | 0.18476844  |\n",
      "| time_elapsed            | 3           |\n",
      "| total timesteps         | 887         |\n",
      "| value_loss              | 0.100501545 |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| ent_coef                | 0.7765449  |\n",
      "| ent_coef_loss           | -0.3911495 |\n",
      "| entropy                 | 1.19262    |\n",
      "| episodes                | 160        |\n",
      "| fps                     | 248        |\n",
      "| mean 100 episode reward | 6.2        |\n",
      "| n_updates               | 850        |\n",
      "| policy_loss             | -4.3259916 |\n",
      "| qf1_loss                | 0.17365511 |\n",
      "| qf2_loss                | 0.20929843 |\n",
      "| time_elapsed            | 3          |\n",
      "| total timesteps         | 950        |\n",
      "| value_loss              | 0.16924894 |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.75105953  |\n",
      "| ent_coef_loss           | -0.44070575 |\n",
      "| entropy                 | 1.1462998   |\n",
      "| episodes                | 170         |\n",
      "| fps                     | 248         |\n",
      "| mean 100 episode reward | 6.9         |\n",
      "| n_updates               | 968         |\n",
      "| policy_loss             | -4.5646057  |\n",
      "| qf1_loss                | 0.3055378   |\n",
      "| qf2_loss                | 0.37408242  |\n",
      "| time_elapsed            | 4           |\n",
      "| total timesteps         | 1068        |\n",
      "| value_loss              | 0.25537616  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.7212212   |\n",
      "| ent_coef_loss           | -0.43574965 |\n",
      "| entropy                 | 1.0230067   |\n",
      "| episodes                | 180         |\n",
      "| fps                     | 251         |\n",
      "| mean 100 episode reward | 7.9         |\n",
      "| n_updates               | 1117        |\n",
      "| policy_loss             | -5.214609   |\n",
      "| qf1_loss                | 0.3111645   |\n",
      "| qf2_loss                | 0.3540251   |\n",
      "| time_elapsed            | 4           |\n",
      "| total timesteps         | 1217        |\n",
      "| value_loss              | 0.2769451   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.6926505   |\n",
      "| ent_coef_loss           | -0.48225537 |\n",
      "| entropy                 | 0.91744775  |\n",
      "| episodes                | 190         |\n",
      "| fps                     | 253         |\n",
      "| mean 100 episode reward | 9           |\n",
      "| n_updates               | 1274        |\n",
      "| policy_loss             | -5.95745    |\n",
      "| qf1_loss                | 0.41313875  |\n",
      "| qf2_loss                | 0.49330914  |\n",
      "| time_elapsed            | 5           |\n",
      "| total timesteps         | 1374        |\n",
      "| value_loss              | 0.2814548   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.64734834  |\n",
      "| ent_coef_loss           | -0.51388615 |\n",
      "| entropy                 | 0.7554085   |\n",
      "| episodes                | 200         |\n",
      "| fps                     | 256         |\n",
      "| mean 100 episode reward | 11.3        |\n",
      "| n_updates               | 1549        |\n",
      "| policy_loss             | -7.3113565  |\n",
      "| qf1_loss                | 0.24478272  |\n",
      "| qf2_loss                | 0.3082744   |\n",
      "| time_elapsed            | 6           |\n",
      "| total timesteps         | 1649        |\n",
      "| value_loss              | 0.17410716  |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| ent_coef                | 0.58864945 |\n",
      "| ent_coef_loss           | -0.618398  |\n",
      "| entropy                 | 0.647581   |\n",
      "| episodes                | 210        |\n",
      "| fps                     | 260        |\n",
      "| mean 100 episode reward | 14.8       |\n",
      "| n_updates               | 1948       |\n",
      "| policy_loss             | -8.823396  |\n",
      "| qf1_loss                | 0.76752406 |\n",
      "| qf2_loss                | 0.79730636 |\n",
      "| time_elapsed            | 7          |\n",
      "| total timesteps         | 2048       |\n",
      "| value_loss              | 0.2289525  |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.5162105   |\n",
      "| ent_coef_loss           | -0.67488265 |\n",
      "| entropy                 | 0.5250021   |\n",
      "| episodes                | 220         |\n",
      "| fps                     | 260         |\n",
      "| mean 100 episode reward | 19.6        |\n",
      "| n_updates               | 2504        |\n",
      "| policy_loss             | -10.717045  |\n",
      "| qf1_loss                | 1.0174677   |\n",
      "| qf2_loss                | 1.1253377   |\n",
      "| time_elapsed            | 10          |\n",
      "| total timesteps         | 2604        |\n",
      "| value_loss              | 0.30572122  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.41987887  |\n",
      "| ent_coef_loss           | -0.73137355 |\n",
      "| entropy                 | 0.40932304  |\n",
      "| episodes                | 230         |\n",
      "| fps                     | 252         |\n",
      "| mean 100 episode reward | 27.4        |\n",
      "| n_updates               | 3381        |\n",
      "| policy_loss             | -14.010998  |\n",
      "| qf1_loss                | 0.46778455  |\n",
      "| qf2_loss                | 0.37005997  |\n",
      "| time_elapsed            | 13          |\n",
      "| total timesteps         | 3481        |\n",
      "| value_loss              | 0.17791153  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.3234468   |\n",
      "| ent_coef_loss           | -0.62437594 |\n",
      "| entropy                 | 0.16191441  |\n",
      "| episodes                | 240         |\n",
      "| fps                     | 258         |\n",
      "| mean 100 episode reward | 37.7        |\n",
      "| n_updates               | 4468        |\n",
      "| policy_loss             | -17.810503  |\n",
      "| qf1_loss                | 1.5235951   |\n",
      "| qf2_loss                | 1.4611202   |\n",
      "| time_elapsed            | 17          |\n",
      "| total timesteps         | 4568        |\n",
      "| value_loss              | 0.11290334  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.26379898  |\n",
      "| ent_coef_loss           | -0.74068165 |\n",
      "| entropy                 | -0.03941937 |\n",
      "| episodes                | 250         |\n",
      "| fps                     | 262         |\n",
      "| mean 100 episode reward | 45.7        |\n",
      "| n_updates               | 5358        |\n",
      "| policy_loss             | -21.342329  |\n",
      "| qf1_loss                | 0.62159884  |\n",
      "| qf2_loss                | 0.67049074  |\n",
      "| time_elapsed            | 20          |\n",
      "| total timesteps         | 5458        |\n",
      "| value_loss              | 0.14285961  |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0003       |\n",
      "| ent_coef                | 0.2137069    |\n",
      "| ent_coef_loss           | -0.6519873   |\n",
      "| entropy                 | -0.047364045 |\n",
      "| episodes                | 260          |\n",
      "| fps                     | 259          |\n",
      "| mean 100 episode reward | 54.6         |\n",
      "| n_updates               | 6310         |\n",
      "| policy_loss             | -22.361246   |\n",
      "| qf1_loss                | 1.2152321    |\n",
      "| qf2_loss                | 1.4059477    |\n",
      "| time_elapsed            | 24           |\n",
      "| total timesteps         | 6410         |\n",
      "| value_loss              | 0.13236207   |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| ent_coef                | 0.17195076 |\n",
      "| ent_coef_loss           | -0.4037096 |\n",
      "| entropy                 | -0.1865991 |\n",
      "| episodes                | 270        |\n",
      "| fps                     | 255        |\n",
      "| mean 100 episode reward | 63         |\n",
      "| n_updates               | 7264       |\n",
      "| policy_loss             | -25.974756 |\n",
      "| qf1_loss                | 2.8001182  |\n",
      "| qf2_loss                | 3.0929441  |\n",
      "| time_elapsed            | 28         |\n",
      "| total timesteps         | 7364       |\n",
      "| value_loss              | 0.10548046 |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.14022702  |\n",
      "| ent_coef_loss           | -1.1728505  |\n",
      "| entropy                 | -0.31308806 |\n",
      "| episodes                | 280         |\n",
      "| fps                     | 258         |\n",
      "| mean 100 episode reward | 70.4        |\n",
      "| n_updates               | 8159        |\n",
      "| policy_loss             | -27.722712  |\n",
      "| qf1_loss                | 0.46980855  |\n",
      "| qf2_loss                | 0.5662216   |\n",
      "| time_elapsed            | 31          |\n",
      "| total timesteps         | 8259        |\n",
      "| value_loss              | 0.10434512  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.11185915  |\n",
      "| ent_coef_loss           | -1.0787022  |\n",
      "| entropy                 | -0.10227523 |\n",
      "| episodes                | 290         |\n",
      "| fps                     | 260         |\n",
      "| mean 100 episode reward | 78          |\n",
      "| n_updates               | 9069        |\n",
      "| policy_loss             | -28.15441   |\n",
      "| qf1_loss                | 0.9526582   |\n",
      "| qf2_loss                | 0.9930005   |\n",
      "| time_elapsed            | 35          |\n",
      "| total timesteps         | 9169        |\n",
      "| value_loss              | 0.29626316  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.09370093  |\n",
      "| ent_coef_loss           | 0.23696007  |\n",
      "| entropy                 | -0.40499827 |\n",
      "| episodes                | 300         |\n",
      "| fps                     | 262         |\n",
      "| mean 100 episode reward | 85.4        |\n",
      "| n_updates               | 10092       |\n",
      "| policy_loss             | -32.33352   |\n",
      "| qf1_loss                | 1.0838953   |\n",
      "| qf2_loss                | 1.209226    |\n",
      "| time_elapsed            | 38          |\n",
      "| total timesteps         | 10192       |\n",
      "| value_loss              | 0.2718203   |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0003       |\n",
      "| ent_coef                | 0.086811416  |\n",
      "| ent_coef_loss           | 0.7591705    |\n",
      "| entropy                 | -0.044108663 |\n",
      "| episodes                | 310          |\n",
      "| fps                     | 263          |\n",
      "| mean 100 episode reward | 91.5         |\n",
      "| n_updates               | 11099        |\n",
      "| policy_loss             | -30.233398   |\n",
      "| qf1_loss                | 0.92496073   |\n",
      "| qf2_loss                | 0.94412774   |\n",
      "| time_elapsed            | 42           |\n",
      "| total timesteps         | 11199        |\n",
      "| value_loss              | 0.19964232   |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.0967464   |\n",
      "| ent_coef_loss           | -0.03439276 |\n",
      "| entropy                 | -0.3604573  |\n",
      "| episodes                | 320         |\n",
      "| fps                     | 264         |\n",
      "| mean 100 episode reward | 96.3        |\n",
      "| n_updates               | 12132       |\n",
      "| policy_loss             | -36.45311   |\n",
      "| qf1_loss                | 0.44796312  |\n",
      "| qf2_loss                | 0.45809358  |\n",
      "| time_elapsed            | 46          |\n",
      "| total timesteps         | 12232       |\n",
      "| value_loss              | 0.45424098  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.113596    |\n",
      "| ent_coef_loss           | 0.58740115  |\n",
      "| entropy                 | -0.19267601 |\n",
      "| episodes                | 330         |\n",
      "| fps                     | 265         |\n",
      "| mean 100 episode reward | 97.7        |\n",
      "| n_updates               | 13155       |\n",
      "| policy_loss             | -35.92791   |\n",
      "| qf1_loss                | 0.20147878  |\n",
      "| qf2_loss                | 0.36236924  |\n",
      "| time_elapsed            | 49          |\n",
      "| total timesteps         | 13255       |\n",
      "| value_loss              | 0.1755149   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.12144781  |\n",
      "| ent_coef_loss           | 0.39296907  |\n",
      "| entropy                 | -0.15257151 |\n",
      "| episodes                | 340         |\n",
      "| fps                     | 266         |\n",
      "| mean 100 episode reward | 96.5        |\n",
      "| n_updates               | 14121       |\n",
      "| policy_loss             | -36.212315  |\n",
      "| qf1_loss                | 1.1869799   |\n",
      "| qf2_loss                | 2.0152874   |\n",
      "| time_elapsed            | 53          |\n",
      "| total timesteps         | 14221       |\n",
      "| value_loss              | 0.23259515  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.12729673  |\n",
      "| ent_coef_loss           | 0.2716034   |\n",
      "| entropy                 | -0.08809782 |\n",
      "| episodes                | 350         |\n",
      "| fps                     | 266         |\n",
      "| mean 100 episode reward | 96.6        |\n",
      "| n_updates               | 15017       |\n",
      "| policy_loss             | -36.01652   |\n",
      "| qf1_loss                | 2.3995128   |\n",
      "| qf2_loss                | 2.5878987   |\n",
      "| time_elapsed            | 56          |\n",
      "| total timesteps         | 15117       |\n",
      "| value_loss              | 0.3441654   |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0003       |\n",
      "| ent_coef                | 0.12394846   |\n",
      "| ent_coef_loss           | 0.11789505   |\n",
      "| entropy                 | -0.071855664 |\n",
      "| episodes                | 360          |\n",
      "| fps                     | 264          |\n",
      "| mean 100 episode reward | 97           |\n",
      "| n_updates               | 16012        |\n",
      "| policy_loss             | -35.15751    |\n",
      "| qf1_loss                | 1.060344     |\n",
      "| qf2_loss                | 1.1497394    |\n",
      "| time_elapsed            | 60           |\n",
      "| total timesteps         | 16112        |\n",
      "| value_loss              | 0.23335946   |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.12412566  |\n",
      "| ent_coef_loss           | -0.64600754 |\n",
      "| entropy                 | 0.05785489  |\n",
      "| episodes                | 370         |\n",
      "| fps                     | 264         |\n",
      "| mean 100 episode reward | 97.4        |\n",
      "| n_updates               | 17004       |\n",
      "| policy_loss             | -38.730682  |\n",
      "| qf1_loss                | 0.82427657  |\n",
      "| qf2_loss                | 0.8604058   |\n",
      "| time_elapsed            | 64          |\n",
      "| total timesteps         | 17104       |\n",
      "| value_loss              | 0.24123299  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.12097993  |\n",
      "| ent_coef_loss           | -0.19838919 |\n",
      "| entropy                 | 0.010070261 |\n",
      "| episodes                | 380         |\n",
      "| fps                     | 265         |\n",
      "| mean 100 episode reward | 98.9        |\n",
      "| n_updates               | 18048       |\n",
      "| policy_loss             | -35.987564  |\n",
      "| qf1_loss                | 2.269403    |\n",
      "| qf2_loss                | 3.0235312   |\n",
      "| time_elapsed            | 68          |\n",
      "| total timesteps         | 18148       |\n",
      "| value_loss              | 0.2532312   |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| ent_coef                | 0.12123116 |\n",
      "| ent_coef_loss           | -0.5399944 |\n",
      "| entropy                 | -0.3571922 |\n",
      "| episodes                | 390        |\n",
      "| fps                     | 265        |\n",
      "| mean 100 episode reward | 99.9       |\n",
      "| n_updates               | 19062      |\n",
      "| policy_loss             | -44.007168 |\n",
      "| qf1_loss                | 0.5573455  |\n",
      "| qf2_loss                | 0.59836954 |\n",
      "| time_elapsed            | 72         |\n",
      "| total timesteps         | 19162      |\n",
      "| value_loss              | 0.25078452 |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0003       |\n",
      "| ent_coef                | 0.12513806   |\n",
      "| ent_coef_loss           | -0.053180024 |\n",
      "| entropy                 | 0.022856839  |\n",
      "| episodes                | 400          |\n",
      "| fps                     | 266          |\n",
      "| mean 100 episode reward | 100          |\n",
      "| n_updates               | 20096        |\n",
      "| policy_loss             | -39.12964    |\n",
      "| qf1_loss                | 0.33891004   |\n",
      "| qf2_loss                | 0.38783002   |\n",
      "| time_elapsed            | 75           |\n",
      "| total timesteps         | 20196        |\n",
      "| value_loss              | 0.18980598   |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.12091075  |\n",
      "| ent_coef_loss           | -0.49405363 |\n",
      "| entropy                 | -0.16722493 |\n",
      "| episodes                | 410         |\n",
      "| fps                     | 266         |\n",
      "| mean 100 episode reward | 100         |\n",
      "| n_updates               | 21124       |\n",
      "| policy_loss             | -45.733597  |\n",
      "| qf1_loss                | 0.7049198   |\n",
      "| qf2_loss                | 0.96178895  |\n",
      "| time_elapsed            | 79          |\n",
      "| total timesteps         | 21224       |\n",
      "| value_loss              | 0.2672525   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.10617813  |\n",
      "| ent_coef_loss           | -0.7271421  |\n",
      "| entropy                 | -0.10499498 |\n",
      "| episodes                | 420         |\n",
      "| fps                     | 267         |\n",
      "| mean 100 episode reward | 102         |\n",
      "| n_updates               | 22292       |\n",
      "| policy_loss             | -39.365547  |\n",
      "| qf1_loss                | 0.25708896  |\n",
      "| qf2_loss                | 0.22654599  |\n",
      "| time_elapsed            | 83          |\n",
      "| total timesteps         | 22392       |\n",
      "| value_loss              | 0.13117926  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.10180567  |\n",
      "| ent_coef_loss           | 0.048244476 |\n",
      "| entropy                 | -0.14379007 |\n",
      "| episodes                | 430         |\n",
      "| fps                     | 267         |\n",
      "| mean 100 episode reward | 106         |\n",
      "| n_updates               | 23733       |\n",
      "| policy_loss             | -43.979958  |\n",
      "| qf1_loss                | 2.7415233   |\n",
      "| qf2_loss                | 3.6998239   |\n",
      "| time_elapsed            | 89          |\n",
      "| total timesteps         | 23833       |\n",
      "| value_loss              | 0.2724351   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.10540279  |\n",
      "| ent_coef_loss           | -0.42664337 |\n",
      "| entropy                 | -0.3998693  |\n",
      "| episodes                | 440         |\n",
      "| fps                     | 268         |\n",
      "| mean 100 episode reward | 112         |\n",
      "| n_updates               | 25280       |\n",
      "| policy_loss             | -41.400635  |\n",
      "| qf1_loss                | 1.3362263   |\n",
      "| qf2_loss                | 0.98508763  |\n",
      "| time_elapsed            | 94          |\n",
      "| total timesteps         | 25380       |\n",
      "| value_loss              | 0.16674712  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.111892276 |\n",
      "| ent_coef_loss           | 1.1770656   |\n",
      "| entropy                 | -0.15015867 |\n",
      "| episodes                | 450         |\n",
      "| fps                     | 268         |\n",
      "| mean 100 episode reward | 122         |\n",
      "| n_updates               | 27211       |\n",
      "| policy_loss             | -40.56697   |\n",
      "| qf1_loss                | 0.52750087  |\n",
      "| qf2_loss                | 0.55947906  |\n",
      "| time_elapsed            | 101         |\n",
      "| total timesteps         | 27311       |\n",
      "| value_loss              | 0.30880347  |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0003       |\n",
      "| ent_coef                | 0.11609217   |\n",
      "| ent_coef_loss           | -0.039718524 |\n",
      "| entropy                 | -0.22546816  |\n",
      "| episodes                | 460          |\n",
      "| fps                     | 268          |\n",
      "| mean 100 episode reward | 134          |\n",
      "| n_updates               | 29438        |\n",
      "| policy_loss             | -38.757275   |\n",
      "| qf1_loss                | 0.57048017   |\n",
      "| qf2_loss                | 1.0491713    |\n",
      "| time_elapsed            | 109          |\n",
      "| total timesteps         | 29538        |\n",
      "| value_loss              | 0.6351583    |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.117117815 |\n",
      "| ent_coef_loss           | -0.2275264  |\n",
      "| entropy                 | -0.25939265 |\n",
      "| episodes                | 470         |\n",
      "| fps                     | 268         |\n",
      "| mean 100 episode reward | 158         |\n",
      "| n_updates               | 32752       |\n",
      "| policy_loss             | -44.573853  |\n",
      "| qf1_loss                | 0.22214004  |\n",
      "| qf2_loss                | 0.21737482  |\n",
      "| time_elapsed            | 122         |\n",
      "| total timesteps         | 32852       |\n",
      "| value_loss              | 0.09655272  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.124878734 |\n",
      "| ent_coef_loss           | 0.06094517  |\n",
      "| entropy                 | -0.12621832 |\n",
      "| episodes                | 480         |\n",
      "| fps                     | 269         |\n",
      "| mean 100 episode reward | 247         |\n",
      "| n_updates               | 42752       |\n",
      "| policy_loss             | -53.75441   |\n",
      "| qf1_loss                | 0.83462465  |\n",
      "| qf2_loss                | 1.2644024   |\n",
      "| time_elapsed            | 158         |\n",
      "| total timesteps         | 42852       |\n",
      "| value_loss              | 0.5444622   |\n",
      "-----------------------------------------\n",
      "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
      "Creating window glfw\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stable/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stable/lib/python3.6/site-packages/gym/envs/mujoco/mujoco_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, width, height, camera_id, camera_name)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'human'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_viewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/external/mujoco-py/mujoco_py/mjviewer.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                 \u001b[0mrender_inner_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# Markers and overlay are regenerated in every pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/external/mujoco-py/mujoco_py/mjviewer.py\u001b[0m in \u001b[0;36mrender_inner_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_overlay\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_full_overlay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_video\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_pixels_as_in_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/external/mujoco-py/mujoco_py/mjviewer.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gui_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mglfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmjrendercontext.pyx\u001b[0m in \u001b[0;36mmujoco_py.cymj.MjRenderContextWindow.render\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stable/lib/python3.6/site-packages/glfw/__init__.py\u001b[0m in \u001b[0;36mswap_buffers\u001b[0;34m(window)\u001b[0m\n\u001b[1;32m   2237\u001b[0m \u001b[0m_glfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglfwSwapBuffers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m \u001b[0m_glfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglfwSwapBuffers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOINTER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_GLFWwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2239\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mswap_buffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2240\u001b[0m     \"\"\"\n\u001b[1;32m   2241\u001b[0m     \u001b[0mSwaps\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfront\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mback\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mspecified\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines.sac.policies import MlpPolicy\n",
    "from stable_baselines import SAC\n",
    "\n",
    "env = gym.make('InvertedPendulum-v2')\n",
    "\n",
    "model = SAC(MlpPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=50000, log_interval=10)\n",
    "model.save(\"sac_cartpole\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = SAC.load(\"sac_cartpole\")\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stable Baselines (3.6)",
   "language": "python",
   "name": "stable"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
