{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines.ppo2.ppo2.PPO2 at 0x7fda611dc668>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines import PPO2, TRPO, DDPG, A2C\n",
    "import seagul.envs\n",
    "import gym\n",
    "\n",
    "env_name = 'su_acro_drake-v0'\n",
    "#env_name = 'mj_su_cartpole-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines.ddpg.policies import LnMlpPolicy\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "from stable_baselines import DDPG\n",
    "from stable_baselines.ddpg import AdaptiveParamNoiseSpec\n",
    "\n",
    "\n",
    "best_mean_reward, n_steps = -np.inf, 0\n",
    "\n",
    "def callback(_locals, _globals):\n",
    "  \"\"\"\n",
    "  Callback called at each step (for DQN an others) or after n steps (see ACER or PPO2)\n",
    "  :param _locals: (dict)\n",
    "  :param _globals: (dict)\n",
    "  \"\"\"\n",
    "  global n_steps, best_mean_reward\n",
    "  # Print stats every 1000 calls\n",
    "  if (n_steps + 1) % 1000 == 0:\n",
    "      # Evaluate policy training performance\n",
    "        x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
    "        if len(x) > 0:\n",
    "            mean_reward = np.mean(y[-100:])\n",
    "            print(x[-1], 'timesteps')\n",
    "            print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(best_mean_reward, mean_reward))\n",
    "\n",
    "            # New best model, you could save the agent here\n",
    "            if mean_reward > best_mean_reward:\n",
    "                best_mean_reward = mean_reward\n",
    "                # Example for saving best model\n",
    "                print(\"Saving new best model\")\n",
    "                _locals['self'].save(log_dir + 'best_model.pkl')\n",
    "        n_steps += 1\n",
    "        return True\n",
    "\n",
    "\n",
    "env = DummyVecEnv([lambda: env])  # The algorithms require a vectorized environment to run\n",
    "\n",
    "models = []\n",
    "model = PPO2('MlpPolicy', env, \n",
    "             #nb_rollout_steps=500,\n",
    "             #normalize_observations=True,\n",
    "             #batch_size = 512,\n",
    "             verbose=False,\n",
    "            )\n",
    "model.learn(1228800, seed=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "-129.4850025173438\n",
      "174.85784558475308\n"
     ]
    }
   ],
   "source": [
    "myenv = env\n",
    "myenv.num_steps=1000\n",
    "\n",
    "#model.gate_fn.net_fn = gate\n",
    "action_hist = np.zeros((myenv.num_steps,1))\n",
    "state_hist = np.zeros((myenv.num_steps, myenv.observation_space.shape[0]))\n",
    "reward_hist = np.zeros((myenv.num_steps, 1))\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "num_trials = 100\n",
    "rewards = np.zeros((num_trials))\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    obs = env.reset()\n",
    "    for i in range(myenv.num_steps):\n",
    "            actions, _states, = model.predict(obs)\n",
    "            #actions = np.ones(1)*100\n",
    "            obs, reward, done, _ = env.step(actions)        \n",
    "            action_hist[i,:] = np.copy(actions)\n",
    "            state_hist[i,:] = np.copy(obs)\n",
    "            reward_hist[i,:] = np.copy(reward)\n",
    "            env.render()\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    rewards[trial] = sum(reward_hist)\n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "# plt.plot(action_hist)\n",
    "# plt.title('actions')\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(reward_hist)\n",
    "# plt.title('reward')\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(state_hist[:,0])\n",
    "# plt.title(\"theta\")\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(state_hist[:,1])\n",
    "# plt.title(\"x\")\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(state_hist[:,2])\n",
    "# plt.title(\"theta dot\")\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(state_hist[:,3])\n",
    "# plt.title(\"xdot \")\n",
    "\n",
    "\n",
    "\n",
    "print(i)\n",
    "#print(sum(reward_hist))\n",
    "print(rewards.mean())\n",
    "print(rewards.std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import pi\n",
    "\n",
    "def control(env, q):\n",
    "    k1 = 140.560\n",
    "    k2 = -3.162\n",
    "    k3 = 41.772\n",
    "    k4 = 0#-8.314\n",
    "    u = -4*(k1 * (q[0] - pi) + k2 * q[1] + k3 * q[2] + k4 * q[3])\n",
    "    return u\n",
    "\n",
    "myenv = env\n",
    "myenv.num_steps=400\n",
    "\n",
    "#model.gate_fn.net_fn = gate\n",
    "action_hist = np.zeros((myenv.num_steps,1))\n",
    "state_hist = np.zeros((myenv.num_steps, myenv.observation_space.shape[0]))\n",
    "reward_hist = np.zeros((myenv.num_steps, 1))\n",
    "path_hist = np.zeros((myenv.num_steps,1))\n",
    "\n",
    "obs = env.reset()\n",
    "#env.state[0] = np.pi\n",
    "model.hyst_state = 0\n",
    "\n",
    "for i in range(myenv.num_steps):\n",
    "        if ((155 * pi/180 < obs[0][0] < 220 * pi/180)):\n",
    "            actions = np.array(control(env, obs[0]))/50\n",
    "            #actions = np.zeros(1)\n",
    "            path_hist[i]  = 1\n",
    "        \n",
    "        else:\n",
    "            actions, _,  = model.predict(obs)\n",
    "            path_hist[i] = 0\n",
    "            \n",
    "        \n",
    "        actions = np.clip(actions, -75, 75)\n",
    "        #actions = np.zeros(1)\n",
    "        obs, reward, done, _ = env.step(np.array(actions).reshape(-1,1))\n",
    "        action_hist[i,:] = np.copy(actions)\n",
    "        state_hist[i,:] = np.copy(obs)\n",
    "        reward_hist[i,:] = np.copy(reward)\n",
    "        #env.render()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "plt.plot(action_hist)\n",
    "plt.title('action')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(path_hist)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(state_hist[:,0])\n",
    "plt.title(\"theta\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(state_hist[:,1])\n",
    "plt.title(\"x\")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(state_hist[:,2])\n",
    "plt.title(\"theta dot\")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(state_hist[:,3])\n",
    "plt.title(\"xdot \")\n",
    "\n",
    "print(i)\n",
    "print(sum(reward_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1967.46878566,     0.        ,     0.        ,     0.        ],\n",
       "       [-1975.04432526,     0.        ,     0.        ,     0.        ],\n",
       "       [-1960.63329954,     0.        ,     0.        ,     0.        ],\n",
       "       [-1959.69436482,     0.        ,     0.        ,     0.        ],\n",
       "       [-1952.9140735 ,     0.        ,     0.        ,     0.        ],\n",
       "       [-1971.14081908,     0.        ,     0.        ,     0.        ],\n",
       "       [-1952.04835808,     0.        ,     0.        ,     0.        ],\n",
       "       [-1893.88650594,     0.        ,     0.        ,     0.        ],\n",
       "       [-1966.49728884,     0.        ,     0.        ,     0.        ],\n",
       "       [-1949.22009965,     0.        ,     0.        ,     0.        ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda3",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
