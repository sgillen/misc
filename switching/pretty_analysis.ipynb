{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import seagul.envs\n",
    "gym.make('su_acro_drake-v0')\n",
    "\n",
    "import pickle\n",
    "import torch.utils.data\n",
    "\n",
    "from seagul.rl.run_utils import load_model, load_workspace\n",
    "import numpy as np\n",
    "from numpy import pi\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from seagul.rl.models import PPOModel, SwitchedPPOModel, SwitchedPPOModelActHold\n",
    "from seagul.nn import MLP\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 12\n",
    "BIGGER_SIZE = 18\n",
    "\n",
    "# plt.rc('font', size=BIGGER_SIZE)          # controls default text sizes\n",
    "# plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "# plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "# plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "# plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "# plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "# plt.rc('figure', titlesize=MEDIUM_SIZE)  # fontsize of the figure title\n",
    "\n",
    "#load_path = './data/mj_baseline7/acrobot73380899'\n",
    "#load_path = './data/inv_pend/acrobot_116_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seagul.rl.run_utils\n",
      "seagul.rl.run_utils\n",
      "seagul.rl.run_utils\n",
      "seagul.rl.run_utils\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1024000,) (409,) (1024000,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-88128c00ea46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_workspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mavg_rewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'raw_rew_hist'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'raw_rew_hist'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m#append(ws['avg_reward_hist'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1024000,) (409,) (1024000,) "
     ]
    }
   ],
   "source": [
    "def control(env,q):\n",
    "    k = array([[1316.85000612,  555.41763935,  570.32667002,  272.57631536]])\n",
    "    goal = np.copy(env.state)\n",
    "    goal[0] -= pi\n",
    "    return -k.dot(goal)\n",
    "\n",
    "\n",
    "#seeds = [6,7,8,9]; load_paths =  ['./data/drake_acro_switch2/500_nhb' + str(seed) for seed in seeds]\n",
    "#seeds = [0,1,2,3]; load_paths =  ['./data/drake_acro_switch_final/1000_nhb_se' + str(seed) for seed in seeds]\n",
    "#seeds = [6,7,8,9]; load_paths =  ['./data/drake_acro_final/seed' + str(seed) for seed in seeds]\n",
    "#seeds = [0,1,2,3]; load_paths =  ['./data/acrobot6/acrobot_gamma1_' + str(seed) for seed in seeds]\n",
    "\n",
    "#seeds = [0]; load_paths =  ['/home/sgillen/work/seagul/seagul/notebooks/switching2/data/drake_ppo2/25_ppo20/' + str(seed) for seed in seeds]\n",
    "#seeds = [0]; load_paths =  ['/home/sgillen/work/seagul/seagul/notebooks/switching2/data/drake_ppo2/25_ppo20/' + str(seed) for seed in seeds]\n",
    "\n",
    "\n",
    "\n",
    "seeds = [6,7,8,9]; load_pseeds = [6,7,8,9]; load_paths =  ['/home/sgillen/work/seagul/seagul/notebooks/switching2/data/drake_ppo2/25_ppo20/']\n",
    "\n",
    "\n",
    "#seeds = [0]; load_paths = ['./data/drake_acro_final/warm_seed' + str(seed) for seed in seeds]\n",
    "model, env, args, ws = load_workspace(load_paths[0])\n",
    "num_steps = ws['total_steps']\n",
    "#num_steps = 1000\n",
    "\n",
    "\n",
    "avg_rewards = np.zeros((num_steps,))\n",
    "rewards = np.zeros((num_steps,4))\n",
    "\n",
    "for i, load_path in enumerate(load_paths):\n",
    "    model, env, args, ws = load_workspace(load_path)\n",
    "    \n",
    "    avg_rewards += np.array((ws['raw_rew_hist']))\n",
    "    rewards[:,i] = np.array((ws['raw_rew_hist']))\n",
    "    #append(ws['avg_reward_hist'])\n",
    "    #plt.title('reward vs epoch')\n",
    "    #plt.figure()\n",
    "    #plt.plot(ws['a'])\n",
    "    #plt.figure()\n",
    "    #plt.plot(ws['p_loss_hist'])\n",
    "    #plt.figure()\n",
    "    #plt.plot(ws['v_loss_hist'])\n",
    "    #print(ws['num_states'])\n",
    "    \n",
    "avg_rewards /= len(seeds)\n",
    "min_rewards = [np.min(rewards[i,:]) for i in range(num_steps)]\n",
    "max_rewards = [np.max(rewards[i,:]) for i in range(num_steps)]\n",
    "plt.plot(avg_rewards)\n",
    "plt.plot(min_rewards)\n",
    "plt.plot(max_rewards)\n",
    "\n",
    "rewards_smoothed = pd.Series(avg_rewards).rolling(10, min_periods=10).mean()\n",
    "min_smoothed =  pd.Series(min_rewards).rolling(10, min_periods=10).mean()\n",
    "max_smoothed =  pd.Series(max_rewards).rolling(10, min_periods=10).mean()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(rewards_smoothed)\n",
    "plt.plot(min_smoothed)\n",
    "plt.plot(max_smoothed)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "epochs = [t for t in range(num_steps)]\n",
    "plt.plot(rewards_smoothed[1:1200], color='k')\n",
    "plt.fill_between(epochs[1:1200], min_smoothed[1:1200], max_smoothed[1:1200], color='k',alpha=.2 )\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average rewards')\n",
    "plt.title('Reward curve, Vanilla PPO')\n",
    "\n",
    "#plt.figure()\n",
    "#plt.plot(np.array(ws['ep_path_tensor']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def control(env,q):\n",
    "    k = array([[1316.85000612,  555.41763935,  570.32667002,  272.57631536]])\n",
    "    goal = np.copy(env.state)\n",
    "    goal[0] -= pi\n",
    "    return -k.dot(goal)\n",
    "\n",
    "\n",
    "# Acrobot\n",
    "#load_path = './data/drake_acro4/seed7'\n",
    "load_path = load_paths[0]\n",
    "model, env, args, ws = load_workspace(load_path)\n",
    "model.env = env\n",
    "myenv = env\n",
    "\n",
    "action_hist = np.zeros((myenv.num_steps,1))\n",
    "state_hist = np.zeros((myenv.num_steps, myenv.observation_space.shape[0]))\n",
    "reward_hist = np.zeros((myenv.num_steps, 1))\n",
    "logp_hist = np.zeros((myenv.num_steps, 1))\n",
    "gate_mean = np.zeros((myenv.num_steps,1))\n",
    "\n",
    "#model.action_var = 0\n",
    "#model.action_var = 1\n",
    "#model.gate_var = 0\n",
    "\n",
    "#model.action_var = 0\n",
    "obs = myenv.reset(init_vec = [0,0,0,0])\n",
    "\n",
    "for i in range(myenv.num_steps):\n",
    "        actions, _, _, logp = model.step(obs)\n",
    "        #actions = np.clip(actions, -700,700)\n",
    "        gate_mean[i,:] = model.gate_fn(torch.as_tensor(obs)).detach().numpy()\n",
    "        #actions = np.zeros(1)\n",
    "        #actions = np.clip(actions,-10, 10)\n",
    "        obs, reward, done, _ = env.step(actions)        \n",
    "        action_hist[i,:] = np.copy(actions)\n",
    "        state_hist[i,:] = np.copy(obs)\n",
    "        reward_hist[i,:] = np.copy(reward)\n",
    "        try:\n",
    "            logp_hist[i,:]   = logp.detach()\n",
    "        except:\n",
    "            logp_hist[i,:]   = logp\n",
    "            \n",
    "        #env.render()\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def control(env,q):\n",
    "    k = array([[1316.85000612,  555.41763935,  570.32667002,  272.57631536]])\n",
    "    goal = np.copy(env.state)\n",
    "    goal[0] -= pi\n",
    "    return -k.dot(goal)\n",
    "\n",
    "\n",
    "# Acrobot\n",
    "#load_path = './data/drake_acro4/seed7'\n",
    "load_path  = './data/drake_acro_switch2/500_nhb7'\n",
    "\n",
    "#load_path = load_paths[0]\n",
    "model, env, args, ws = load_workspace(load_path)\n",
    "\n",
    "#model.policy = DummyNet(4,1, 0,0,0)\n",
    "#model.policy = torch.load('./policy_warm_final')\n",
    "model.env = env\n",
    "myenv = env\n",
    "#myenv.num_steps=1000\n",
    "#myenv.max_t = 5\n",
    "\n",
    "\n",
    "#model.gate_fn.net_fn = gate\n",
    "action_hist = np.zeros((myenv.num_steps,1))\n",
    "state_hist = np.zeros((myenv.num_steps, myenv.observation_space.shape[0]))\n",
    "reward_hist = np.zeros((myenv.num_steps, 1))\n",
    "logp_hist = np.zeros((myenv.num_steps, 1))\n",
    "gate_mean = np.zeros((myenv.num_steps,1))\n",
    "\n",
    "#model.action_var = 0\n",
    "#model.action_var = 1\n",
    "#model.gate_var = 0\n",
    "\n",
    "#model.action_var = 0\n",
    "obs = env.reset(init_vec=[0,0,0,0])\n",
    "\n",
    "for i in range(myenv.num_steps):\n",
    "        actions, _, _, logp = model.step(obs)\n",
    "        #actions = np.clip(actions, -700,700)\n",
    "        gate_mean[i,:] = model.gate_fn(torch.as_tensor(obs)).detach().numpy()\n",
    "        #actions = np.zeros(1)\n",
    "        #actions = np.clip(actions,-10, 10)\n",
    "        obs, reward, done, _ = env.step(actions)        \n",
    "        action_hist[i,:] = np.copy(actions)\n",
    "        state_hist[i,:] = np.copy(obs)\n",
    "        reward_hist[i,:] = np.copy(reward)\n",
    "        try:\n",
    "            logp_hist[i,:]   = logp.detach()\n",
    "        except:\n",
    "            logp_hist[i,:]   = logp\n",
    "            \n",
    "        #env.render()\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "t_end = int(env.num_steps)\n",
    "\n",
    "gate_hist = [1 if p == 0 else 0 for p in logp_hist]\n",
    "t = [.005*i for i in range(myenv.num_steps)]\n",
    "plt.plot(t[:t_end], action_hist[:t_end], color='k')\n",
    "plt.title('Action during episode')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Action (Nm)')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(t, np.clip(action_hist, -5,5), color='k')\n",
    "plt.title('Action during episod/e clipped')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Action (N*m)')\n",
    "\n",
    "plt.figure()\n",
    "actions_smoothed = pd.Series(action_hist.squeeze()).rolling(5, min_periods=5).mean()\n",
    "plt.plot(t, actions_smoothed, color='k')\n",
    "plt.title('Smoothed action during episode')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Action (N*m)')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(t[:t_end], reward_hist[:t_end], color='k')\n",
    "plt.title('Reward (pole height)')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Reward')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(t[:t_end], gate_mean[:t_end] ,'x', color='k')\n",
    "plt.title('Gate function mean')\n",
    "#plt.yticks([0,1],['MLP', 'LQR'])\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Mean')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(t[:t_end], gate_hist[:t_end], 'o', color='k', alpha=.05,markersize=16)\n",
    "plt.title('Path through controller')\n",
    "plt.yticks([0,1],['MLP', 'LQR'])\n",
    "plt.xlabel('Time (seconds)')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(state_hist[:,0])\n",
    "plt.title(\"th1\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(state_hist[:,1])\n",
    "plt.title(\"th2\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(state_hist[:,2])\n",
    "plt.title(\"th1_dot\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(state_hist[:,3])\n",
    "plt.title(\"th2_dot)\")\n",
    "\n",
    "print(i)\n",
    "print(sum(reward_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import sin,cos\n",
    "\n",
    "\n",
    "#load_path = './data/drake_acro_switch2/500_nhb8'\n",
    "#load_path = load_paths[0]\n",
    "#load_path = torch.load()\n",
    "model, env, args, ws = load_workspace(load_path)\n",
    "#model, env, = load_model(load_path)\n",
    "\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 12\n",
    "BIGGER_SIZE = 18\n",
    "\n",
    "plt.rc('font', size=BIGGER_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=MEDIUM_SIZE)  # fontsize of the figure title\n",
    "\n",
    "model.gate_fn = torch.load('gate_fn_dr')\n",
    "model.policy = torch.load('warm_policy_dr')\n",
    "\n",
    "n_thdot = 1\n",
    "n_th = 400\n",
    "fig, ax = plt.subplots(n_thdot,n_thdot, figsize=(16,16))\n",
    "\n",
    "th1_vals = np.linspace(0,2*pi,n_th)\n",
    "th2_vals = np.linspace(-pi,pi,n_th)\n",
    "\n",
    "th1dot_vals = np.linspace(-10,10,n_thdot)\n",
    "th2dot_vals = np.linspace(-30,30,n_thdot)\n",
    "\n",
    "for th1i, th1dot in enumerate(th1dot_vals):\n",
    "    for th2i, th2dot in enumerate(th2dot_vals):\n",
    "       \n",
    "        means = np.zeros((n_th,n_th))\n",
    "\n",
    "        for i,th1 in enumerate(th1_vals):\n",
    "            for j,th2 in enumerate(th2_vals):\n",
    "                #obs = torch.tensor([cos(th1), sin(th1), cos(th2), sin(th2), th1dot,th2dot])\n",
    "                obs = torch.tensor([th1, th2, th1dot, th2dot])\n",
    "                means[i,j] = model.gate_fn(obs)\n",
    "\n",
    "\n",
    "\n",
    "        # generate 2 2d grids for the x & y bounds\n",
    "        y, x = np.meshgrid(th1_vals, th2_vals)\n",
    "        z = means\n",
    "\n",
    "        # x and y are bounds, so z should be the value *inside* those bounds.\n",
    "        # Therefore, remove the last value from the z array.\n",
    "        z = z[:-1, :-1]\n",
    "        z_min, z_max = z.min(), z.max()\n",
    "\n",
    "\n",
    "        c = ax.pcolormesh(x, y, z, cmap='RdBu', vmin=z_min, vmax=z_max)\n",
    "        #ax.set_title('pcolormesh')\n",
    "        # set the limits of the plot to the limits of the data\n",
    "        ax.axis([x.min(), x.max(), y.min(), y.max()])\n",
    "\n",
    "        #plt.show()\n",
    "        \n",
    "#for i,a in enumerate(ax):\n",
    "    #for j,b in enumerate(a):\n",
    "        #b.set(ylabel = \"theta 1\", xlabel=\"theta 2\", title=\"th2d=\" + str(th2dot_vals[j]) +  \"  th1d=\" + str(th1dot_vals[i]) )\n",
    "        #b.label_outer()\n",
    "fig.colorbar(c, ax=ax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 24\n",
    "BIGGER_SIZE = 24\n",
    "import matplotlib\n",
    "\n",
    "plt.rc('font', size=BIGGER_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=MEDIUM_SIZE)  # fontsize of the figure title\n",
    "\n",
    "#matplotlib.rc('font', **font)\n",
    "\n",
    "fig, ax = plt.subplots(n_thdot,n_thdot, figsize=(10,8))\n",
    "\n",
    "\n",
    "y, x = np.meshgrid(th1_vals, th2_vals)\n",
    "z = means\n",
    "\n",
    "# x and y are bounds, so z should be the value *inside* those bounds.\n",
    "# Therefore, remove the last value from the z array.\n",
    "z = z[:-1, :-1]\n",
    "z_min, z_max = 0, np.abs(z).max()\n",
    "\n",
    "\n",
    "c = ax.pcolormesh(x, y, z, cmap='RdBu', vmin=z_min, vmax=z_max)\n",
    "#ax.set_title('pcolormesh')\n",
    "# set the limits of the plot to the limits of the data\n",
    "ax.axis([x.min(), x.max(), y.min(), y.max()])\n",
    "fig.colorbar(c, ax=ax)\n",
    "    \n",
    "plt.xlabel(r'$\\theta_{2}$')\n",
    "plt.ylabel(r'$\\theta_{1}$')\n",
    "plt.title(r'Pretrained gate network output for $\\dot \\theta_{1} = \\dot \\theta_{2} = 0$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import sin,cos\n",
    "\n",
    "\n",
    "\n",
    "load_path = './data/drake_acro4/5'\n",
    "model, env, args, ws = load_workspace(load_path)\n",
    "\n",
    "\n",
    "#model.gate_fn = torch.load(\"./gate_fn_drv\")\n",
    "\n",
    "n_thdot = 5\n",
    "n_th = 100\n",
    "fig, ax = plt.subplots(n_thdot,n_thdot, figsize=(22,16))\n",
    "\n",
    "th1_vals = np.linspace(0,2*pi,n_th)\n",
    "th2_vals = np.linspace(-pi,pi,n_th)\n",
    "\n",
    "th1dot_vals = np.linspace(-10,10,n_thdot)\n",
    "th2dot_vals = np.linspace(-30,30,n_thdot)\n",
    "\n",
    "for th1i, th1dot in enumerate(th1dot_vals):\n",
    "    for th2i, th2dot in enumerate(th2dot_vals):\n",
    "       \n",
    "        means = np.zeros((n_th,n_th))\n",
    "\n",
    "        for i,th1 in enumerate(th1_vals):\n",
    "            for j,th2 in enumerate(th2_vals):\n",
    "                #obs = torch.tensor([cos(th1), sin(th1), cos(th2), sin(th2), th1dot,th2dot])\n",
    "                obs = torch.tensor([th1, th2, th1dot, th2dot])\n",
    "                means[i,j] = model.gate_fn(obs)\n",
    "\n",
    "\n",
    "\n",
    "        # generate 2 2d grids for the x & y bounds\n",
    "        y, x = np.meshgrid(th1_vals, th2_vals)\n",
    "        z = means\n",
    "\n",
    "        # x and y are bounds, so z should be the value *inside* those bounds.\n",
    "        # Therefore, remove the last value from the z array.\n",
    "        z = z[:-1, :-1]\n",
    "        z_min, z_max = 0, np.abs(z).max()\n",
    "\n",
    "\n",
    "        c = ax[th1i, th2i].pcolormesh(x, y, z, cmap='RdBu', vmin=z_min, vmax=z_max)\n",
    "        #ax.set_title('pcolormesh')\n",
    "        # set the limits of the plot to the limits of the data\n",
    "        ax[th1i, th2i].axis([x.min(), x.max(), y.min(), y.max()])\n",
    "\n",
    "        #plt.show()\n",
    "        \n",
    "for i,a in enumerate(ax):\n",
    "    for j,b in enumerate(a):\n",
    "        b.set(ylabel = \"theta 1\", xlabel=\"theta 2\", title=\"theta 2 dot = \" + str(th2dot_vals[i]))\n",
    "        b.label_outer()\n",
    "        \n",
    "fig.colorbar(c, ax=ax)\n",
    "\n",
    "\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Cartpole\n",
    "\n",
    "myenv = env\n",
    "myenv.num_steps=1500 \n",
    "\n",
    "sss\n",
    "\n",
    "#model.gate_fn.net_fn = gate\n",
    "action_hist = np.zeros((myenv.num_steps,1))\n",
    "state_hist = np.zeros((myenv.num_steps, myenv.observation_space.shape[0]))\n",
    "reward_hist = np.zeros((myenv.num_steps, 1))\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "for i in range(myenv.num_steps):\n",
    "        actions, _, _, _ = model.step(obs)\n",
    "        #actions = np.zeros(1)\n",
    "        obs, reward, done, _ = env.step(actions)        \n",
    "        action_hist[i,:] = np.copy(actions)\n",
    "        state_hist[i,:] = np.copy(obs)\n",
    "        reward_hist[i,:] = np.copy(reward)\n",
    "        #env.render()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "plt.plot(action_hist)\n",
    "plt.figure()\n",
    "plt.plot(reward_hist)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(state_hist[:,0])\n",
    "plt.title(\"theta\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(state_hist[:,1])\n",
    "plt.title(\"x\")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(state_hist[:,2])\n",
    "plt.title(\"theta dot\")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(state_hist[:,3])\n",
    "plt.title(\"xdot \")\n",
    "\n",
    "\n",
    "print(i)\n",
    "print(sum(reward_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "load_path  = './data/drake_acro_switch2/500_nhb7'\n",
    "model, env, args, ws = load_workspace(load_path)\n",
    "\n",
    "model.env = env\n",
    "myenv = env\n",
    "\n",
    "\n",
    "num_trials = 1\n",
    "\n",
    "action_hist = np.zeros((num_trials, myenv.num_steps,1))\n",
    "state_hist = np.zeros((num_trials, myenv.num_steps, myenv.observation_space.shape[0]))\n",
    "reward_hist = np.zeros((num_trials, myenv.num_steps, 1))\n",
    "logp_hist = np.zeros((num_trials, myenv.num_steps, 1))\n",
    "gate_mean = np.zeros((num_trials, myenv.num_steps,1))\n",
    "init_state_hist = np.zeros((num_trials,myenv.observation_space.shape[0]))\n",
    "\n",
    "for t in range(num_trials):\n",
    "    init_state = np.random.random(4)*.01\n",
    "    centers = np.array([.5,0,.5,.5])\n",
    "    weights = np.array([0,0,0,0])\n",
    "    init_state = (init_state-centers)*weights\n",
    "    \n",
    "    init_state_hist[t] = init_state\n",
    "    obs = env.reset(init_vec = init_state)\n",
    "    for i in range(env.num_steps):\n",
    "            actions, _, _, logp = model.step(obs)\n",
    "            obs, reward, done, _ = env.step(actions)        \n",
    "            action_hist[t,i,:] = np.copy(actions)\n",
    "            state_hist[t,i,:] = np.copy(obs)\n",
    "            reward_hist[t,i,:] = np.copy(reward)\n",
    "            try:\n",
    "                logp_hist[t,i,:]   = logp.detach()\n",
    "            except:\n",
    "                logp_hist[t,i,:]   = logp\n",
    "            \n",
    "            #env.render()\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(init_state_hist)\n",
    "print(sum(sum(reward_hist)))\n",
    "print(state_hist[t,-1,0],state_hist[t,-1,1])\n",
    "\n",
    "\n",
    "for t in range(num_trials):\n",
    "    plt.plot(np.cos(state_hist[t,:,0]) + np.cos(state_hist[t,:,0] + state_hist[t,:,1]) , np.sin(state_hist[t,:,0]) + np.sin(state_hist[t,:,0] + state_hist[t,:,1]))\n",
    "    plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Drake (3.6)",
   "language": "python",
   "name": "drake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
