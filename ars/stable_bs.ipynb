{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/sgillen/work/third_party/stable-baselines/stable_baselines/common/tf_util.py:57: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sgillen/work/third_party/stable-baselines/stable_baselines/common/tf_util.py:66: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sgillen/work/third_party/stable-baselines/stable_baselines/common/policies.py:115: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sgillen/work/third_party/stable-baselines/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sgillen/work/third_party/stable-baselines/stable_baselines/common/policies.py:560: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/sgillen/miniconda3/envs/stable/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/sgillen/work/third_party/stable-baselines/stable_baselines/a2c/utils.py:156: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sgillen/work/third_party/stable-baselines/stable_baselines/common/distributions.py:418: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sgillen/work/third_party/stable-baselines/stable_baselines/ppo2/ppo2.py:194: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sgillen/work/third_party/stable-baselines/stable_baselines/ppo2/ppo2.py:202: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sgillen/miniconda3/envs/stable/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/sgillen/work/third_party/stable-baselines/stable_baselines/ppo2/ppo2.py:210: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sgillen/work/third_party/stable-baselines/stable_baselines/ppo2/ppo2.py:244: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sgillen/work/third_party/stable-baselines/stable_baselines/ppo2/ppo2.py:246: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines.ppo2.ppo2.PPO2 at 0x7f3fd8258978>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines import PPO2, DDPG, A2C\n",
    "import seagul.envs\n",
    "import gym\n",
    "import pybullet_envs\n",
    "\n",
    "#env_name = 'Walker2d-v2'\n",
    "env_name = \"Walker2DBulletEnv-v0\"\n",
    "#env_name = 'su_acro_drake-v0'\n",
    "#env_name = 'mj_su_cartpole-v0'\n",
    "#env_name = \"InvertedPendulum-v2\"\n",
    "env = gym.make(env_name)\n",
    "\n",
    "\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines.ddpg.policies import LnMlpPolicy\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "from stable_baselines import DDPG\n",
    "from stable_baselines.ddpg import AdaptiveParamNoiseSpec\n",
    "\n",
    "best_mean_reward, n_steps = -np.inf, 0\n",
    "\n",
    "def callback(_locals, _globals):\n",
    "  \"\"\"\n",
    "  Callback called at each step (for DQN an others) or after n steps (see ACER or PPO2)\n",
    "  :param _locals: (dict)\n",
    "  :param _globals: (dict)\n",
    "  \"\"\"\n",
    "  global n_steps, best_mean_reward\n",
    "  # Print stats every 1000 calls\n",
    "  if (n_steps + 1) % 1000 == 0:\n",
    "      # Evaluate policy training performance\n",
    "        x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
    "        if len(x) > 0:\n",
    "            mean_reward = np.mean(y[-100:])\n",
    "            print(x[-1], 'timesteps')\n",
    "            print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(best_mean_reward, mean_reward))\n",
    "\n",
    "            # New best model, you could save the agent here\n",
    "            if mean_reward > best_mean_reward:\n",
    "                best_mean_reward = mean_reward\n",
    "                # Example for saving best model\n",
    "                print(\"Saving new best model\")\n",
    "                _locals['self'].save(log_dir + 'best_model.pkl')\n",
    "        n_steps += 1\n",
    "        return True\n",
    "\n",
    "\n",
    "env = DummyVecEnv([lambda: env])  # The algorithms require a vectorized environment to run\n",
    "\n",
    "models = []\n",
    "model = PPO2('MlpPolicy', env, \n",
    "             #nb_rollout_steps=500,\n",
    "             #normalize_observations=True,\n",
    "             #batch_size = 512,\n",
    "             verbose=False,\n",
    "            )\n",
    "model.learn(int(2e6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myenv = env\n",
    "myenv.num_steps=1000\n",
    "env.render(mode=\"human\")\n",
    "\n",
    "\n",
    "#model.gate_fn.net_fn = gate\n",
    "action_hist = np.zeros((myenv.num_steps,6))\n",
    "state_hist = np.zeros((myenv.num_steps, myenv.observation_space.shape[0]))\n",
    "reward_hist = np.zeros((myenv.num_steps, 1))\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "num_trials = 10\n",
    "rewards = np.zeros((num_trials))\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    obs = env.reset()\n",
    "    for i in range(myenv.num_steps):\n",
    "            actions, _states, = model.predict(obs)\n",
    "            #actions = np.ones(1)*100\n",
    "            obs, reward, done, _ = env.step(actions)        \n",
    "            action_hist[i,:] = np.copy(actions)\n",
    "            state_hist[i,:] = np.copy(obs)\n",
    "            reward_hist[i,:] = np.copy(reward)\n",
    "            env.render()\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    rewards[trial] = sum(reward_hist)\n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "plt.plot(action_hist)\n",
    "plt.title('actions')\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(reward_hist)\n",
    "# plt.title('reward')\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(state_hist[:,0])\n",
    "# plt.title(\"theta\")\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(state_hist[:,1])\n",
    "# plt.title(\"x\")\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(state_hist[:,2])\n",
    "# plt.title(\"theta dot\")\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(state_hist[:,3])\n",
    "# plt.title(\"xdot \")\n",
    "\n",
    "\n",
    "\n",
    "print(i)\n",
    "#print(sum(reward_hist))\n",
    "print(rewards.mean())\n",
    "print(rewards.std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import pi\n",
    "\n",
    "def control(env, q):\n",
    "    k1 = 140.560\n",
    "    k2 = -3.162\n",
    "    k3 = 41.772\n",
    "    k4 = 0#-8.314\n",
    "    u = -4*(k1 * (q[0] - pi) + k2 * q[1] + k3 * q[2] + k4 * q[3])\n",
    "    return u\n",
    "\n",
    "myenv = env\n",
    "myenv.num_steps=400\n",
    "\n",
    "#model.gate_fn.net_fn = gate\n",
    "action_hist = np.zeros((myenv.num_steps,1))\n",
    "state_hist = np.zeros((myenv.num_steps, myenv.observation_space.shape[0]))\n",
    "reward_hist = np.zeros((myenv.num_steps, 1))\n",
    "path_hist = np.zeros((myenv.num_steps,1))\n",
    "\n",
    "obs = env.reset()\n",
    "#env.state[0] = np.pi\n",
    "model.hyst_state = 0\n",
    "\n",
    "for i in range(myenv.num_steps):\n",
    "        if ((155 * pi/180 < obs[0][0] < 220 * pi/180)):\n",
    "            actions = np.array(control(env, obs[0]))/50\n",
    "            #actions = np.zeros(1)\n",
    "            path_hist[i]  = 1\n",
    "        \n",
    "        else:\n",
    "            actions, _,  = model.predict(obs)\n",
    "            path_hist[i] = 0\n",
    "            \n",
    "        \n",
    "        actions = np.clip(actions, -75, 75)\n",
    "        #actions = np.zeros(1)\n",
    "        obs, reward, done, _ = env.step(np.array(actions).reshape(-1,1))\n",
    "        action_hist[i,:] = np.copy(actions)\n",
    "        state_hist[i,:] = np.copy(obs)\n",
    "        reward_hist[i,:] = np.copy(reward)\n",
    "        #env.render()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "plt.plot(action_hist)\n",
    "plt.title('action')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(path_hist)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(state_hist[:,0])\n",
    "plt.title(\"theta\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(state_hist[:,1])\n",
    "plt.title(\"x\")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(state_hist[:,2])\n",
    "plt.title(\"theta dot\")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(state_hist[:,3])\n",
    "plt.title(\"xdot \")\n",
    "\n",
    "print(i)\n",
    "print(sum(reward_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stable Baselines (3.6)",
   "language": "python",
   "name": "stable"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
